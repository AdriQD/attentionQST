{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FNZzM4LheAlY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch import distributions\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGRiHfcTeAld"
   },
   "source": [
    "UPload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k_vus6lkeAlh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = 1000\n",
    "url = '../data/Haar4qubitsTrials'+str(trials)+'BIGsics.npy'\n",
    "#url ='/content/drive/MyDrive/data4Ent/5qubitsmulti/5qubitsTrials'+str(trials)+'multinomialSics.npy'\n",
    "#url = '/content/drive/MyDrive/data4Ent/MtoMdata/qutrit'+str(trials)+'RandomSic.npy'\n",
    "#url2 = '/content/drive/MyDrive/data4Ent/q32/gltest_d32_SIC'+str(trials)+'.npy'\n",
    "\n",
    "tomo = np.load(url, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zodin5T96rch",
    "outputId": "3c2193bc-711f-4c1f-b913-a9d8883bc60a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sN2JV_zAeAlj",
    "outputId": "28466989-dced-46cc-952b-5331ae9ce2cb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10050, 512), (1485, 512), (3000, 512))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomo_train, tomo_temp = train_test_split(tomo, test_size=0.33, random_state=1)\n",
    "tomo_valid,tomo_test = train_test_split(tomo_temp, test_size=0.7,random_state=1)\n",
    "\n",
    "\n",
    "import random\n",
    "valid = tomo_valid\n",
    "train = tomo_train\n",
    "test = tomo_test[:3000]\n",
    "\n",
    "random.shuffle(train)\n",
    "random.shuffle(valid)\n",
    "random.shuffle(test)\n",
    "\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P1qhXX-eAlk"
   },
   "source": [
    "##### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CeSIEWDheAlk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StatesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index]\n",
    "\n",
    "train_dataset = StatesDataset(train)\n",
    "evaluation_dataset = StatesDataset(valid)\n",
    "\n",
    "test_dataset = StatesDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyvYi_5neAll"
   },
   "source": [
    "# Model and seed fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "QM9CVrpjhjGo",
    "outputId": "dca31510-79e3-4a40-bd63-95f641524cc7",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Netbeta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(\u001b[43mNetbeta\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Netbeta' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache\n",
    "del(Netbeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XL-In4pzeAln",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Netbeta(nn.Module):\n",
    "    def __init__(self, state_local_d, num_low_triang, out_channel, nheads, dim_ff ):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel = 21\n",
    "        kernel2 = 21\n",
    "        \n",
    "\n",
    "        padd = 10 #modello scon convtranspose era a 24, kernel1 = 49, all strid = 1\n",
    "        strid1 = 1\n",
    "        strid2 = 1\n",
    "        model_dim = int((state_local_d + num_low_triang*2 - kernel + 2*padd)/strid1)+1\n",
    "\n",
    "\n",
    "        #ENCODING\n",
    "        self.conv1 = nn.Conv1d(1, out_channel, kernel_size = kernel, padding = padd,stride = strid1 )\n",
    "        self.conv2 = nn.Conv1d(out_channel, 1, kernel_size = kernel2, padding = padd, stride = strid2 )\n",
    "\n",
    "\n",
    "        #RECONSTRUCTION\n",
    "\n",
    "        self.enc_transf = nn.TransformerEncoderLayer(d_model = model_dim, nhead = nheads, dim_feedforward = dim_ff, batch_first = True, norm_first=True)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.enc_stack = nn.TransformerEncoder(self.enc_transf, num_layers=2)\n",
    "        self.T = torch.nn.Tanh()\n",
    "        self.G = torch.nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #ENCODING\n",
    "\n",
    "        x = self.G(self.conv1(x)) #this for pauli's\n",
    "        #x= F.selu(self.conv1(x))\n",
    "\n",
    "        #DECODING\n",
    "\n",
    "        x= self.G(self.enc_stack(x))\n",
    "\n",
    "        x = self.T(self.conv2(x))\n",
    "        x = torch.flatten(x,1) \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RjIGeJXeAlo"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOJrak9YeAlp"
   },
   "source": [
    "leading parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LE5Y9UR6eAlq",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f66f40655d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "local_dim = 16\n",
    "dim = 1\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfJbr_hLeAlq"
   },
   "source": [
    "network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLePRBqLEqCV"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zd564H1NeAlr",
    "outputId": "0d12742d-14e7-4fb3-a37b-d48c5ae327fd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    }
   ],
   "source": [
    "num_killed_inputs = 0\n",
    "\n",
    "input_dim = local_dim**(2*dim)\n",
    "batch_size = 500\n",
    "\n",
    "statedim = local_dim**dim\n",
    "lowerval = (statedim**2 -statedim)/2\n",
    "print(lowerval)\n",
    "\n",
    "device = \"cuda\"\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "netb = Netbeta(int(local_dim) , int(lowerval), 2,1,20)\n",
    "themodelbeta  = netb.to(device)\n",
    "themodelbeta = themodelbeta.double()\n",
    "\n",
    "\n",
    "def normalization_loss(inp):\n",
    "\n",
    "  batchsize = inp.shape[0]\n",
    "\n",
    "  #return torch.abs(torch.sum(torch.norm(single_vector, p ='fro', dim=1))/batchsize -1 )\n",
    "  return  torch.mean(torch.norm(inp, p ='fro', dim=1))/batchsize\n",
    "\n",
    "def diceCost(inp,target):\n",
    "    \n",
    "    a = torch.sum(torch.mul(inp,target),1)\n",
    "    b = torch.sum(torch.mul(inp,inp),1)\n",
    "    c = torch.sum(torch.mul(target,target),1)\n",
    "\n",
    "    return 1-  torch.mean(torch.div(2*a, (b+c) ))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N811svmUeAlr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    evaluation_dataset, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "sGL-f7NkeAls",
    "outputId": "69a32451-440a-4e68-eda9-79f87c9d4b93",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03997004126188461 0.02272932561214167\n",
      "1 0.019632285835484476 0.01577926550040681\n",
      "2 0.015495198755581524 0.013103984744590555\n",
      "3 0.01290265391819119 0.010961174929890455\n",
      "4 0.010759318706236403 0.009120204486137748\n",
      "5 0.009039740531623826 0.007691092476146146\n",
      "6 0.0076449431707188665 0.006591620811038724\n",
      "7 0.006572356156672209 0.005797687886818908\n",
      "8 0.006000161148757798 0.0054769521735628926\n",
      "9 0.005757746691995093 0.005313597955102775\n",
      "10 0.005579542521687646 0.005186182408474034\n",
      "11 0.005461057993701282 0.005097241518153459\n",
      "12 0.005345233439691535 0.005018414987857803\n",
      "13 0.005266303757406117 0.004929696935589048\n",
      "14 0.005154459122641665 0.004859329666246778\n",
      "15 0.005104061612607323 0.004793306659415464\n",
      "16 0.0050162411723055815 0.0047348600841602825\n",
      "17 0.004954103001920383 0.0046834661777746525\n",
      "18 0.0048826854363047635 0.00462569526425708\n",
      "19 0.004832673555990856 0.004589088379372722\n",
      "20 0.004784739823254928 0.004538742895769609\n",
      "21 0.004721745573048903 0.00449279899485423\n",
      "22 0.0046671408345056635 0.004455882264645724\n",
      "23 0.0046239958029919296 0.004410447000803189\n",
      "24 0.004580519339366688 0.004376632903900404\n",
      "25 0.004547398156227195 0.004346659030124259\n",
      "26 0.00449722750428088 0.004318895234901456\n",
      "27 0.0044705611550055185 0.004280916168211082\n",
      "28 0.004433370160326889 0.004265630346965906\n",
      "29 0.004405307820907522 0.004232667022560825\n",
      "30 0.004359876943590145 0.004215896058791742\n",
      "31 0.004345042921298369 0.004193441412247217\n",
      "32 0.004318932238181133 0.0041627758354050816\n",
      "33 0.004302315372874046 0.004153218886964122\n",
      "34 0.004263757100877984 0.004129935373608944\n",
      "35 0.004246212466038392 0.004113103992207768\n",
      "36 0.00422822044293474 0.004098967333695345\n",
      "37 0.004204959540604629 0.004083590653426614\n",
      "38 0.004192153205838593 0.004071655061442189\n",
      "39 0.0041777467455398854 0.004056410600858312\n",
      "40 0.004156584208901658 0.004047618590608051\n",
      "41 0.0041414056546150895 0.004034900281548311\n",
      "42 0.00413117564678317 0.004022299778684659\n",
      "43 0.004114690255010991 0.00401633564786136\n",
      "44 0.004102305614409221 0.004003940897224439\n",
      "45 0.004092511058455429 0.003995591501463949\n",
      "46 0.004079895594595771 0.003988952812243479\n",
      "47 0.0040695062807790516 0.003978414254709097\n",
      "48 0.004067212196357433 0.0039689661851431725\n",
      "49 0.004053890065564741 0.00396473894805089\n",
      "50 0.004046788544937366 0.003957267005877118\n",
      "51 0.004041637144984541 0.003951964716227568\n",
      "52 0.004034974889407616 0.003945596560257795\n",
      "53 0.004029602415453788 0.003940032216712068\n",
      "54 0.004023545558355483 0.0039296978987993164\n",
      "55 0.004016479742367743 0.003927636073673025\n",
      "56 0.004012260612188016 0.003922846763156738\n",
      "57 0.004005906694251586 0.003916386045939712\n",
      "58 0.004004329517652938 0.003909617132753035\n",
      "59 0.0040023684312135 0.003907420426267682\n",
      "60 0.003989094321216807 0.003897819178866681\n",
      "61 0.003989641575445501 0.0038919843815320796\n",
      "62 0.003984742349517538 0.00389109039325472\n",
      "63 0.003981145947822722 0.003883704762740858\n",
      "64 0.0039858094398553924 0.0038748072873399445\n",
      "65 0.003972490174424713 0.0038742284755681317\n",
      "66 0.003968198457566236 0.0038694743021720937\n",
      "67 0.003967494786037489 0.003866693593334511\n",
      "68 0.003970124885214804 0.0038617011074516074\n",
      "69 0.00396064259007557 0.0038558353207528867\n",
      "70 0.003956414214219432 0.003848262567932612\n",
      "71 0.003954145090273899 0.0038451291016097515\n",
      "72 0.003950380091674264 0.0038406210380885655\n",
      "73 0.0039448646245353524 0.003833294628754612\n",
      "74 0.0039444746766093535 0.0038292460357979403\n",
      "75 0.0039436421058339265 0.0038259473481764756\n",
      "76 0.0039343943768355705 0.003819790233819079\n",
      "77 0.003933298080861759 0.003818548868063893\n",
      "78 0.003927998452698361 0.003813301851606983\n",
      "79 0.003926425402253123 0.003807483592554128\n",
      "80 0.003923401024691065 0.0038035968354464197\n",
      "81 0.003921113594213217 0.003797553272427748\n",
      "82 0.003917168660408231 0.0037924523315934033\n",
      "83 0.003916314852601283 0.003790288569465164\n",
      "84 0.00390729727616323 0.003787767102564427\n",
      "85 0.0039071885833494744 0.003781053654065757\n",
      "86 0.003905791797653127 0.003782506111089053\n",
      "87 0.003903891954149207 0.0037726847645940718\n",
      "88 0.003906140954872331 0.0037668365821078003\n",
      "89 0.003897139922915344 0.003764975441936291\n",
      "90 0.0039015780403040953 0.003760594404930371\n",
      "91 0.0038965525144195865 0.0037553458540085494\n",
      "92 0.0038877558721719654 0.0037508912085744855\n",
      "93 0.0038874257742224096 0.0037519448489560197\n",
      "94 0.003878265638296196 0.003750381022914887\n",
      "95 0.0038867889086857703 0.00374325047984227\n",
      "96 0.003877589084961429 0.003740684239552165\n",
      "97 0.0038752088177396904 0.003736188130206142\n",
      "98 0.003867567867075796 0.0037296972672324763\n",
      "99 0.003873138323068925 0.003729555534741151\n",
      "100 0.0038685126839595038 0.0037263545591798585\n",
      "101 0.003866582284376768 0.003726816922117735\n",
      "102 0.0038584637448209674 0.003718564787134821\n",
      "103 0.0038698049909982705 0.0037189749167864214\n",
      "104 0.0038598516288224196 0.003716437361879285\n",
      "105 0.003860907467222254 0.003718020123756296\n",
      "106 0.0038706169600021055 0.003711122318666629\n",
      "107 0.003852678445936896 0.0037085863421085414\n",
      "108 0.0038588276782672492 0.003702540049316879\n",
      "109 0.003846513219542367 0.003702995888385382\n",
      "110 0.003846491824797194 0.0036978697751938543\n",
      "111 0.0038521125076953454 0.003695630851347224\n",
      "112 0.003850758406361852 0.003701666668983994\n",
      "113 0.0038399815016400907 0.003691923192428741\n",
      "114 0.003842341641233268 0.0036920138058372134\n",
      "115 0.003842102911130219 0.0036916410250098786\n",
      "116 0.00384082671491605 0.003689050597178667\n",
      "117 0.00383833923629922 0.0036852924755529943\n",
      "118 0.003837150576458114 0.003684041122058661\n",
      "119 0.003826864777201855 0.003681701339269041\n",
      "120 0.003833804843826322 0.003677616867004786\n",
      "121 0.003827857743629924 0.0036744955108732047\n",
      "122 0.0038323044443580186 0.003678087282748984\n",
      "123 0.0038283984339596844 0.003670147259064929\n",
      "124 0.0038297542345011276 0.0036740168708527788\n",
      "125 0.003825285939680827 0.0036675415644296266\n",
      "126 0.0038231010904125626 0.003669757893312241\n",
      "127 0.003831426622850617 0.003664743110062697\n",
      "128 0.0038325318015903762 0.0036671178312264272\n",
      "129 0.0038235963784752523 0.003661902872686008\n",
      "130 0.0038264692100499717 0.0036573718032808654\n",
      "131 0.0038198077129573305 0.0036575865839328747\n",
      "132 0.003811542968723971 0.0036558274165007913\n",
      "133 0.003817643666906986 0.003648771330311358\n",
      "134 0.003822986023153505 0.0036528859444534835\n",
      "135 0.003816095917243146 0.0036535403357758228\n",
      "136 0.0038093168534761865 0.003647590436889983\n",
      "137 0.0038174104155413373 0.0036496151095763963\n",
      "138 0.0038068598165112593 0.003643442706605333\n",
      "139 0.0038080109838120125 0.003648055626343776\n",
      "140 0.003820894051558162 0.003643420210230295\n",
      "141 0.003808229410887598 0.003644318375296447\n",
      "142 0.003802815515972128 0.003638878987523759\n",
      "143 0.0038082012184976746 0.00363925423786852\n",
      "144 0.0038020936213101703 0.003637436382564961\n",
      "145 0.0038047885007547098 0.00363959922113999\n",
      "146 0.00380398360188127 0.003633549881101672\n",
      "147 0.003797841416778135 0.003634468759144916\n",
      "148 0.0038087419613707283 0.0036330167045511333\n",
      "149 0.0037962635641251206 0.0036358592686949963\n",
      "150 0.0038014992275586562 0.0036333670465993097\n",
      "151 0.0037977413767599387 0.0036329333149154783\n",
      "152 0.00379585718465394 0.0036297707185646774\n",
      "153 0.003791642886405371 0.003630424372851578\n",
      "154 0.003797418020173525 0.0036247617358701255\n",
      "155 0.0037940297300004524 0.003619235782252171\n",
      "156 0.003787205012950649 0.003624992297321076\n",
      "157 0.003785961309159806 0.003623133376712955\n",
      "158 0.003786634784683476 0.003621142031692049\n",
      "159 0.0037880329175089176 0.00361551347119468\n",
      "160 0.0037956684899307985 0.0036192355993978076\n",
      "161 0.0037949754479519792 0.0036154341868410524\n",
      "162 0.0037962963641918718 0.0036168167490805876\n",
      "163 0.003782919165468148 0.0036160826702798417\n",
      "164 0.0037927826863252234 0.0036095363769118173\n",
      "165 0.0037786967020634436 0.003610893396714509\n",
      "166 0.0037740303064681658 0.0036067813574811682\n",
      "167 0.0037820916339749043 0.0036085054379678123\n",
      "168 0.003793665601418384 0.0036105043100550806\n",
      "169 0.0037921471975761603 0.0036076665122387005\n",
      "170 0.003774085555425253 0.0036018435568475325\n",
      "171 0.003771104175511243 0.0036030105697350124\n",
      "172 0.0037758927624006026 0.0036076104501375577\n",
      "173 0.0037849833087979902 0.003598396147528429\n",
      "174 0.0037759746212208366 0.0036039567341083264\n",
      "175 0.003774947370309313 0.003600056676315296\n",
      "176 0.0037761356079661595 0.003599121503679019\n",
      "177 0.0037744480863616944 0.003598635691417603\n",
      "178 0.003772866264724276 0.00359853239963641\n",
      "179 0.0037705216166759514 0.003597190179977271\n",
      "180 0.0037668920376469524 0.0036016919466797317\n",
      "181 0.0037630953858150116 0.0035974470854477623\n",
      "182 0.003778669473021457 0.003597242467577416\n",
      "183 0.003764739585408047 0.0035920763205129053\n",
      "184 0.003764242268762994 0.0035936347011035624\n",
      "185 0.003760679015730193 0.0035859305321114653\n",
      "186 0.0037704860977076397 0.0035851713993057554\n",
      "187 0.0037562729481010913 0.0035864270607810695\n",
      "188 0.0037600169872454847 0.0035867035249145655\n",
      "189 0.003764626327047893 0.003587557947495099\n",
      "190 0.003765629633932317 0.0035838471028548594\n",
      "191 0.003758298092846378 0.003578359646627026\n",
      "192 0.0037597622900810987 0.0035826195211564097\n",
      "193 0.0037739053940941086 0.0035830263319299328\n",
      "194 0.0037660755532301017 0.0035783002005599276\n",
      "195 0.0037641125709531537 0.003584567300994429\n",
      "196 0.0037606572016163375 0.0035762571122416307\n",
      "197 0.003758917326159262 0.0035708214715929804\n",
      "198 0.0037605582736093663 0.0035792813801539904\n",
      "199 0.003749623045610166 0.0035715830045839093\n",
      "200 0.0037525653177567027 0.003573814738110914\n",
      "201 0.0037592928387088173 0.003573059175073275\n",
      "202 0.0037702868018423506 0.003572209724159458\n",
      "203 0.0037512808941429212 0.003570794766572576\n",
      "204 0.0037532813063032707 0.0035720558543556424\n",
      "205 0.003749549643234956 0.003563923322051847\n",
      "206 0.003741433200575029 0.003569102400515101\n",
      "207 0.00374797727497796 0.003571183053819111\n",
      "208 0.0037471929635439945 0.003567998690736604\n",
      "209 0.003756442595363309 0.0035637771206051545\n",
      "210 0.0037392748121129736 0.0035647175473310557\n",
      "211 0.0037494464121273696 0.0035637649303344693\n",
      "212 0.0037549139163516386 0.0035638188913449218\n",
      "213 0.003744842963655213 0.00356280200379097\n",
      "214 0.003749066519599454 0.003557702167090292\n",
      "215 0.0037422368161124063 0.0035597400643967102\n",
      "216 0.003740331119744108 0.0035602948491034515\n",
      "217 0.0037450700850090294 0.0035621222175363358\n",
      "218 0.0037391405237697153 0.0035576878858128227\n",
      "219 0.0037446270772176103 0.003553869235350171\n",
      "220 0.0037465785362477866 0.003558114639150432\n",
      "221 0.003728877081906799 0.0035583137685815096\n",
      "222 0.0037307808646533094 0.003554243796145809\n",
      "223 0.003737104911892266 0.0035471338312558424\n",
      "224 0.0037377042930363535 0.0035410365509202595\n",
      "225 0.003728225169021855 0.0035531935727303892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m rec_loss(out, target) \u001b[38;5;241m+\u001b[39m normalization_loss(out)\n\u001b[1;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 29\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudaml2/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudaml2/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/cudaml2/lib/python3.11/site-packages/torch/optim/rprop.py:106\u001b[0m, in \u001b[0;36mRprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m     maximize \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params, grads, prevs, step_sizes)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mrprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/cudaml2/lib/python3.11/site-packages/torch/optim/rprop.py:205\u001b[0m, in \u001b[0;36mrprop\u001b[0;34m(params, grads, prevs, step_sizes, foreach, maximize, differentiable, step_size_min, step_size_max, etaminus, etaplus)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rprop\n\u001b[0;32m--> 205\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaminus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaminus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43metaplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metaplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudaml2/lib/python3.11/site-packages/torch/optim/rprop.py:305\u001b[0m, in \u001b[0;36m_multi_tensor_rprop\u001b[0;34m(params, grads, prevs, step_sizes, step_size_min, step_size_max, etaminus, etaplus, maximize, differentiable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     sign[sign\u001b[38;5;241m.\u001b[39mgt(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m etaplus\n\u001b[1;32m    304\u001b[0m     sign[sign\u001b[38;5;241m.\u001b[39mlt(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m etaminus\n\u001b[0;32m--> 305\u001b[0m     sign[sign\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# update stepsizes with step size updates\u001b[39;00m\n\u001b[1;32m    308\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(grouped_step_sizes, signs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rec_loss = torch.nn.MSELoss()\n",
    "#optimizer = optim.RMSprop(themodelbeta.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Rprop(themodelbeta.parameters(), lr = learning_rate, step_sizes = (1e-6,1e-4))\n",
    "\n",
    "\n",
    "val_hist = []\n",
    "train_hist = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for inputs in train_dataloader:\n",
    "\n",
    "        themodelbeta.train()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        #INPUTS RESHAPING\n",
    "\n",
    "        newin = torch.reshape(inputs[:,:local_dim**(2*dim)  ], (inputs.shape[0],1,local_dim**(2*dim )) )\n",
    "        target = inputs[:,local_dim**2:]\n",
    "        out = themodelbeta(newin)\n",
    "\n",
    "        loss = rec_loss(out, target) + normalization_loss(out)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    valid_loss = 0\n",
    "    for inputs in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            themodelbeta.train()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            target = inputs[:,local_dim**2:]\n",
    "\n",
    "\n",
    "            #INPUTS RESHAPING\n",
    "\n",
    "            newin = torch.reshape(inputs[:,:local_dim**(2*dim)  ], (inputs.shape[0],1,local_dim**(2*dim )) )\n",
    "            out = themodelbeta(newin)\n",
    "\n",
    "            loss = rec_loss(out, target) + normalization_loss(out)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "\n",
    "    print(epoch, train_loss/len(train_dataloader), valid_loss/len(valid_dataloader))\n",
    "    val_hist.append(valid_loss/len(valid_dataloader))\n",
    "    train_hist.append(train_loss/len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSNTsFMgeAls"
   },
   "source": [
    "# Test. HS and fidelity reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7M9WfHMeAlt"
   },
   "source": [
    "## matrix reconstruction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wveIo4E73_FF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xf6_BjnneAlt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_matrix_elements(stuff,d,local_dim):\n",
    "\n",
    "\tdiag_len = local_dim**d\n",
    "\tdiagel = stuff[local_dim**(2*d) : local_dim**(2*d)  + diag_len  ]\n",
    "\toffd = stuff[ local_dim**(2*d)  + diag_len :  ]\n",
    "\treturn diagel, offd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cf8cSl7EeAlt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebuild_last(diags,offd,ind):\n",
    "    '''''\n",
    "    input.\n",
    "    diags : diags elements\n",
    "    offds : offdiagonal elements\n",
    "    rhoshape : just the dm dimension, e.g. number of rows\n",
    "    localDim : the number of level of each particle\n",
    "    numParticles : the total number of particles for tensor product\n",
    "\n",
    "    output.\n",
    "    m : reconstructed cholesky decoposition matrix\n",
    "    '''''\n",
    "    d = len(diags)\n",
    "    eye = np.eye(d,d)\n",
    "    mat = np.zeros((d,d),dtype = complex)\n",
    "\n",
    "    offvalues = [ (a+1j*b) for (a,b) in zip(offd[:int(len(offd)/2 ) ], offd[int(len(offd)/2):])  ]\n",
    "    mat[ind[0], ind[1]] = offvalues\n",
    "\n",
    "    return mat + eye*diags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqRPvuzneAlu"
   },
   "source": [
    "## test stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXsfNYk6eAlu",
    "outputId": "3c5cacd6-104a-45c9-ccb9-3b9251fc9a57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amacarone/miniconda3/envs/cudaml2/lib/python3.11/site-packages/qutip/__init__.py:96: UserWarning: matplotlib not found: Graphics will not work.\n",
      "  warnings.warn(\"matplotlib not found: Graphics will not work.\")\n"
     ]
    }
   ],
   "source": [
    "#just generate ones the index for the recosntruction using a dull matrix\n",
    "\n",
    "import qutip as qt\n",
    "def fid(a,b):\n",
    "  fid = qt.fidelity(qt.Qobj(a), qt.Qobj(b))\n",
    "  return torch.tensor(fid)\n",
    "\n",
    "\n",
    "#parameters\n",
    "qfid =0\n",
    "j=0\n",
    "hs = []\n",
    "fids =[]\n",
    "\n",
    "# generate support diagonal matrix for reconstruction\n",
    "e = np.eye(local_dim**(dim),local_dim**(dim))\n",
    "print(e.shape)\n",
    "\n",
    "#indeces for the reconstruction function\n",
    "ind = np.tril_indices_from(e,k=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vVueaI9jIEs6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bruteForce(dm):\n",
    "\n",
    "\teis, eigvecs = np.linalg.eig(dm)\n",
    "\n",
    "\teis[eis <0 ] = 0.00001\n",
    "\teis = [ el.real for el in eis]\n",
    "\teis = np.array(eis)/sum(eis)\n",
    "\n",
    "\tcleanRho = eigvecs@ np.diag(eis) @ eigvecs.T.conj()\n",
    "\n",
    "\treturn cleanRho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wuFJ2SCPeAlu",
    "outputId": "36698142-7474-4f50-ed48-9027f2f261a6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averaged quantum fidelity  0.6113 \n",
      "0.13399513\n",
      "hilbert schmidt distance average  0.7281\n",
      "hilbert schmidt distance std  0.2522\n"
     ]
    }
   ],
   "source": [
    "file_ = []\n",
    "originals = []\n",
    "with torch.no_grad():\n",
    "  for inputs in test_dataloader:\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    #out_d is the diagonal, out_tr the triangluar values, real and complex altogether\n",
    "    newin = torch.reshape(inputs[:,:local_dim**(2*dim) ], (inputs.shape[0],1,local_dim**2) )\n",
    "\n",
    "    out = themodelbeta(newin)\n",
    "\n",
    "    for i in range(out.shape[0]):\n",
    "\n",
    "      #reconstructing cholesky from dataset input array. NO MORE NEEDED\n",
    "      diagel, offd = return_matrix_elements( inputs[i].cpu().numpy(), dim, local_dim)\n",
    "\n",
    "      original_chol = rebuild_last(diagel, offd , ind)\n",
    "\n",
    "      #reconstructing cholesky from neural network outputs\n",
    "\n",
    "      nn_out = out[i].cpu().numpy()\n",
    "      nn_diag = nn_out[:local_dim]\n",
    "      nn_offd = nn_out[local_dim: local_dim**2]\n",
    "\n",
    "      nn_chol = rebuild_last(nn_diag, nn_offd, ind)\n",
    "\n",
    "      cholo=nn_chol@ nn_chol.conj().T\n",
    "      norm = np.trace(cholo)\n",
    "\n",
    "\n",
    "      #fidelity between reconstructed and originals\n",
    "\n",
    "      fids.append(fid(original_chol@ original_chol.conj().T,cholo/norm) )\n",
    "      hs.append(qt.hilbert_dist(qt.Qobj(cholo/norm), qt.Qobj(original_chol@ original_chol.conj().T)))\n",
    "\n",
    "      file_.append(cholo/norm)\n",
    "      originals.append(original_chol@ original_chol.conj().T)\n",
    "\n",
    "print(f\"averaged quantum fidelity {np.mean(fids): .4f} \")#dont forget the j to divide by\n",
    "\n",
    "print(np.std(fids))\n",
    "print(f\"hilbert schmidt distance average {np.mean(hs): .4f}\")\n",
    "print(f\"hilbert schmidt distance std {np.std(hs): .4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO0qPFBnSrdW"
   },
   "source": [
    "## file saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zG5RtFOG0fpP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(themodelbeta, 'modelsHAAR4qubits'+str(trials)+'-FID97Pauli.pth' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdCMoI4MeAlv"
   },
   "source": [
    "## model delating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATqUvVU3l2Mp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toN6Z6EYeAlw"
   },
   "source": [
    "## third party data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8SSfZgfeAlw"
   },
   "outputs": [],
   "source": [
    "test_gl = gl\n",
    "\n",
    "gl_dataset = StatesDataset(test_gl)\n",
    "gl_dataloader = torch.utils.data.DataLoader(\n",
    "    gl_dataset, batch_size = batch_size, shuffle =False,\n",
    "    pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DntuegVqeAlw"
   },
   "outputs": [],
   "source": [
    "glFile = []\n",
    "\n",
    "name = 'q32TestSet_SICtrials_'+str(trials)+'v2.npy'\n",
    "\n",
    "with torch.no_grad():\n",
    "  for inputs in gl_dataloader:\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    #out_d is the diagonal, out_tr the triangluar values, real and complex altogether\n",
    "    newin = torch.reshape(inputs[:,: ], (inputs.shape[0],1,local_dim**2) )\n",
    "\n",
    "    out_d, out_tr = themodelbeta(newin)\n",
    "\n",
    "    for i in range(out_d.shape[0]):\n",
    "\n",
    "      #reconstructing cholesky from neural network outputs\n",
    "\n",
    "      nn_diag = out_d[i].cpu().numpy()\n",
    "      nn_offd = out_tr[i,:int(out_tr.shape[1])].cpu().numpy()\n",
    "\n",
    "      nn_chol = rebuild_last(nn_diag, nn_offd, ind)\n",
    "\n",
    "      cholo=nn_chol@ nn_chol.conj().T\n",
    "      norm = np.trace(cholo)\n",
    "\n",
    "      glFile.append(cholo/norm)\n",
    "\n",
    "  np.save(name, np.array(glFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wrtdc20E_5LH"
   },
   "outputs": [],
   "source": [
    "PATH = '/content/drive/MyDrive/models5qubitsBest/500Kfid812multinomial.pth'\n",
    "\n",
    "m = torch.load(PATH)\n",
    "m.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
